(.venv) Gabriel@MacBook-Air-de-Joao legisai-scrape % scrapy crawl CamaraOther
2025-06-30 15:47:06 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: Spiders)
2025-06-30 15:47:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.12.0 (v3.12.0:0fb18b02c8, Oct  2 2023, 09:45:56) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 24.0.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.5, Platform macOS-15.3.1-arm64-arm-64bit
2025-06-30 15:47:06 [scrapy.addons] INFO: Enabled addons:
[]
2025-06-30 15:47:06 [asyncio] DEBUG: Using selector: KqueueSelector
2025-06-30 15:47:06 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-06-30 15:47:06 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2025-06-30 15:47:06 [scrapy.extensions.telnet] INFO: Telnet Password: 2020e54d787c7360
2025-06-30 15:47:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapeops_scrapy.extension.ScrapeOpsMonitor']
2025-06-30 15:47:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spiders',
 'DOWNLOAD_DELAY': 2,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'FEED_STORAGE_S3_ACL': 'public-read',
 'NEWSPIDER_MODULE': 'Spiders.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'SPIDER_MODULES': ['Spiders.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) '
               'AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}
2025-06-30 15:47:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapeops_scrapy.middleware.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-06-30 15:47:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from creating-client-class.iot-data to creating-client-class.iot-data-plane
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from before-call.apigateway to before-call.api-gateway
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from request-created.machinelearning.Predict to request-created.machine-learning.Predict
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from before-parameter-build.autoscaling.CreateLaunchConfiguration to before-parameter-build.auto-scaling.CreateLaunchConfiguration
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from before-parameter-build.route53 to before-parameter-build.route-53
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from request-created.cloudsearchdomain.Search to request-created.cloudsearch-domain.Search
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from docs.*.autoscaling.CreateLaunchConfiguration.complete-section to docs.*.auto-scaling.CreateLaunchConfiguration.complete-section
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from before-parameter-build.logs.CreateExportTask to before-parameter-build.cloudwatch-logs.CreateExportTask
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from docs.*.logs.CreateExportTask.complete-section to docs.*.cloudwatch-logs.CreateExportTask.complete-section
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from before-parameter-build.cloudsearchdomain.Search to before-parameter-build.cloudsearch-domain.Search
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Changing event name from docs.*.cloudsearchdomain.Search.complete-section to docs.*.cloudsearch-domain.Search.complete-section
2025-06-30 15:47:06 [botocore.utils] DEBUG: IMDS ENDPOINT: http://169.254.169.254/
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: env
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: assume-role
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: assume-role-with-web-identity
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: sso
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: shared-credentials-file
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: custom-process
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: config-file
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: ec2-credentials-file
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: boto-config
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: container-role
2025-06-30 15:47:06 [botocore.credentials] DEBUG: Looking for credentials via: iam-role
2025-06-30 15:47:06 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 169.254.169.254:80
2025-06-30 15:47:06 [botocore.utils] DEBUG: Caught retryable HTTP exception while making metadata service request to http://169.254.169.254/latest/api/token: Could not connect to the endpoint URL: "http://169.254.169.254/latest/api/token"
Traceback (most recent call last):
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
OSError: [Errno 65] No route to host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/httpsession.py", line 464, in send
    urllib_response = conn.urlopen(
                      ^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 525, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/awsrequest.py", line 96, in request
    rval = super().request(method, url, body, headers, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/awsrequest.py", line 123, in _send_output
    self.send(msg)
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/awsrequest.py", line 223, in send
    return super().send(str)
           ^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <botocore.awsrequest.AWSHTTPConnection object at 0x10317fda0>: Failed to establish a new connection: [Errno 65] No route to host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/utils.py", line 456, in _fetch_metadata_token
    response = self._session.send(request.prepare())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/httpsession.py", line 493, in send
    raise EndpointConnectionError(endpoint_url=request.url, error=e)
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: "http://169.254.169.254/latest/api/token"
2025-06-30 15:47:06 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (2): 169.254.169.254:80
2025-06-30 15:47:06 [botocore.utils] DEBUG: Caught retryable HTTP exception while making metadata service request to http://169.254.169.254/latest/meta-data/iam/security-credentials/: Could not connect to the endpoint URL: "http://169.254.169.254/latest/meta-data/iam/security-credentials/"
Traceback (most recent call last):
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 95, in create_connection
    raise err
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
OSError: [Errno 65] No route to host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/httpsession.py", line 464, in send
    urllib_response = conn.urlopen(
                      ^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 799, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 525, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/packages/six.py", line 770, in reraise
    raise value
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 416, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/awsrequest.py", line 96, in request
    rval = super().request(method, url, body, headers, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 244, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1319, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1365, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1314, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/awsrequest.py", line 123, in _send_output
    self.send(msg)
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/awsrequest.py", line 223, in send
    return super().send(str)
           ^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py", line 1018, in send
    self.connect()
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 205, in connect
    conn = self._new_conn()
           ^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <botocore.awsrequest.AWSHTTPConnection object at 0x103ae5bb0>: Failed to establish a new connection: [Errno 65] No route to host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/utils.py", line 509, in _get_request
    response = self._session.send(request.prepare())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/httpsession.py", line 493, in send
    raise EndpointConnectionError(endpoint_url=request.url, error=e)
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: "http://169.254.169.254/latest/meta-data/iam/security-credentials/"
2025-06-30 15:47:06 [botocore.utils] DEBUG: Max number of attempts exceeded (1) when attempting to retrieve data from metadata service.
2025-06-30 15:47:06 [botocore.loaders] DEBUG: Loading JSON file: /Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/data/endpoints.json
2025-06-30 15:47:06 [botocore.loaders] DEBUG: Loading JSON file: /Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/data/sdk-default-configuration.json
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Event choose-service-name: calling handler <function handle_service_name_alias at 0x10374bba0>
2025-06-30 15:47:06 [botocore.loaders] DEBUG: Loading JSON file: /Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/data/s3/2006-03-01/service-2.json
2025-06-30 15:47:06 [botocore.loaders] DEBUG: Loading JSON file: /Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/data/s3/2006-03-01/endpoint-rule-set-1.json.gz
2025-06-30 15:47:06 [botocore.loaders] DEBUG: Loading JSON file: /Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/data/partitions.json
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x1036afc40>
2025-06-30 15:47:06 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x1036af9c0>
2025-06-30 15:47:06 [botocore.configprovider] DEBUG: Looking for endpoint for s3 via: environment_service
2025-06-30 15:47:06 [botocore.configprovider] DEBUG: Looking for endpoint for s3 via: environment_global
2025-06-30 15:47:06 [botocore.configprovider] DEBUG: Looking for endpoint for s3 via: config_service
2025-06-30 15:47:06 [botocore.configprovider] DEBUG: Looking for endpoint for s3 via: config_global
2025-06-30 15:47:06 [botocore.configprovider] DEBUG: No configured endpoint found.
2025-06-30 15:47:06 [botocore.endpoint] DEBUG: Setting s3 timeout as (60, 60)
2025-06-30 15:47:06 [botocore.loaders] DEBUG: Loading JSON file: /Users/Gabriel/Documents/GitHub/intersites-scrape/.venv/lib/python3.12/site-packages/botocore/data/_retry.json
2025-06-30 15:47:06 [botocore.client] DEBUG: Registering retry handlers for service: s3
2025-06-30 15:47:06 [botocore.utils] DEBUG: Registering S3 region redirector handler
2025-06-30 15:47:06 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy.pipelines.files.FilesPipeline']
2025-06-30 15:47:06 [scrapy.core.engine] INFO: Spider opened
2025-06-30 15:47:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-06-30 15:47:06 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): api.scrapeops.io:443
2025-06-30 15:47:07 [urllib3.connectionpool] DEBUG: https://api.scrapeops.io:443 "POST /api/v1/setup/ HTTP/1.1" 200 None
2025-06-30 15:47:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6029
2025-06-30 15:47:07 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): api.scrapeops.io:443
2025-06-30 15:47:08 [urllib3.connectionpool] DEBUG: https://api.scrapeops.io:443 "POST /api/v1/normalizer/domain/?domain=camara.leg.br HTTP/1.1" 200 244
2025-06-30 15:47:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.camara.leg.br/deputados/204379> (referer: None)
2025-06-30 15:47:09 [scrapy.core.engine] INFO: Closing spider (Todos do gabinete foram coletados.)
2025-06-30 15:47:09 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): api.scrapeops.io:443
2025-06-30 15:47:10 [urllib3.connectionpool] DEBUG: https://api.scrapeops.io:443 "POST /api/v1/stats/ HTTP/1.1" 200 108
2025-06-30 15:47:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 299,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 21529,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 2.660972,
 'finish_reason': 'Todos do gabinete foram coletados.',
 'finish_time': datetime.datetime(2025, 6, 30, 18, 47, 9, 197446, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 182577,
 'httpcompression/response_count': 1,
 'log_count/DEBUG': 55,
 'log_count/INFO': 10,
 'memusage/max': 88834048,
 'memusage/startup': 88834048,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 6, 30, 18, 47, 6, 536474, tzinfo=datetime.timezone.utc)}
2025-06-30 15:47:10 [scrapy.core.engine] INFO: Spider closed (Todos do gabinete foram coletados.)
(.venv) Gabriel@MacBook-Air-de-Joao legisai-scrape % 